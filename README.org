#+TITLE: Camera VLM App - Real-time Vision AI
#+AUTHOR: Amit Thakur
#+DATE: 2025-08-27

* What is this?

This is a simple React component which does image recognition using local vision LLM . The frontend app captures frames from your camera and sends them to a local vision language model. 
AI then describes what it sees. 

Everything runs locally on your machine. No cloud APIs, no monthly subscriptions, no privacy concerns.

Inspired by https://github.ngxson.com/smolvlm-realtime-webcam/ but designed to work with your favorite local models.

* What you'll need

Don't worry - this is easier than it sounds:

- Node.js 18+ (for the web app)
- [[https://ollama.com/][Ollama]] (this will run your AI models)
- A decent computer (4GB+ RAM, GPU helps but isn't required)
- A webcam inbuilt or connected to your computer (obviously!)

* The easy way (recommended for everyone)

We're going to use SmolVLM - it's like having a tiny but smart AI that can see. The best part is it's only 2GB and works great even without a fancy GPU.

** Step 1: Get Ollama running

First, [[https://ollama.com/download][download and install Ollama]] if you haven't already. It's like Docker but for AI models.

** Step 2: Download the SmolVLM model

This is where the magic happens. We'll grab a pre-optimized version:

#+BEGIN_SRC bash
# Download the model file (about 2GB - grab a coffee!)
wget https://huggingface.co/ggml-org/SmolVLM-Instruct-GGUF/resolve/main/SmolVLM-Instruct-Q4_K_M.gguf

# Tell Ollama about our new model
cat > Modelfile.smolvlm <<EOF
FROM SmolVLM-Instruct-Q4_K_M.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
EOF

# Create the model in Ollama
ollama create smolvlm -f Modelfile.smolvlm
#+END_SRC

** Step 3: Try some alternatives (if you want)

If SmolVLM doesn't work for you or you want to try something different:

#+BEGIN_SRC bash
# The classics that everyone loves
ollama pull llava:13b      # Bigger, more capable, but slower
ollama pull moondream      # Another lightweight option
ollama pull llava:latest   # The latest and greatest

# Even smaller SmolVLM variants (if you're really tight on resources)
# wget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-Q4_K_M.gguf
# wget https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/SmolVLM-256M-Instruct-Q4_K_M.gguf
#+END_SRC

* Setting up the app

** Get the code

#+BEGIN_SRC bash
git clone <your-repo-url>
cd local-llm-camera-app
#+END_SRC

** Install dependencies

#+BEGIN_SRC bash
npm install
#+END_SRC

That's it! No Python virtual environments, no complicated setup. Just Node.js doing its thing.



* Let's make it work!

** Fire up Ollama

#+BEGIN_SRC bash
# Start the Ollama service (keep this running in one terminal)
ollama serve
#+END_SRC

** Test your model

#+BEGIN_SRC bash
# In another terminal, make sure your model works
ollama run smolvlm
# Try typing: "Hello, can you see?" 
# If it responds, you're golden! Press Ctrl+D to exit.
#+END_SRC

** Start the web app

#+BEGIN_SRC bash
# Back in your app folder
npm run dev
#+END_SRC

** Open it up

Head to http://localhost:5173 in your browser. You should see your camera feed and some controls.

*Pro tip:* Make sure you allow camera permissions when your browser asks!

* Using the app

Once everything is running, here's what you can play with:

- *Base API*: Should be =http://localhost:11434= (Ollama's default)
- *Model*: Pick =smolvlm= if you followed along, or whatever model you chose
- *Instruction*: Try "What do you see?" or get creative!
- *Interval*: Start with 1 second. SmolVLM is fast, so you can go down to 0.5s if you want

The app has some neat camera controls:
- *Pause*: Stops the AI analysis but keeps your camera on
- *Stop & Close Camera*: Completely releases the camera (good for privacy!)
- You can restart the camera anytime when you're not actively processing

* When things go wrong (troubleshooting)

** "Camera won't work"
- Make sure you're using =http://localhost= (not some weird file:// URL)
- Check your browser permissions - it might be blocking camera access
- Try refreshing the page after allowing permissions

** "Can't connect to model"
- Check if Ollama is running: =ollama list= should show your models
- Try =curl http://localhost:11434/api/tags= to see if Ollama is responding
- Make sure your firewall isn't being overly protective of port 11434

** "Model download failed"
Try the old-school way:
#+BEGIN_SRC bash
# Download manually
curl -L -o SmolVLM-Instruct-Q4_K_M.gguf \
  "https://huggingface.co/ggml-org/SmolVLM-Instruct-GGUF/resolve/main/SmolVLM-Instruct-Q4_K_M.gguf"

# Or use git if you have it set up with LFS
git lfs install
git clone https://huggingface.co/ggml-org/SmolVLM-Instruct-GGUF
#+END_SRC

** "Ollama model creation failed"
Let's try the simple approach:
#+BEGIN_SRC bash
# Check if your GGUF file is there and readable
ls -la SmolVLM-Instruct-Q4_K_M.gguf

# Try a super basic Modelfile
cat > Modelfile.simple <<EOF
FROM SmolVLM-Instruct-Q4_K_M.gguf
EOF

ollama create smolvlm-simple -f Modelfile.simple
#+END_SRC

** "Everything is slow/broken"
- Make sure you have enough RAM (8GB+ recommended)
- Try the smaller SmolVLM variants (256M or 500M instead of 2B)
- Increase the interval between requests to 2-3 seconds
- Consider switching to =moondream= if SmolVLM doesn't work well

* What's in the box?

#+BEGIN_SRC
.
├── index.html          # The main web page
├── package.json        # Node.js stuff (dependencies and scripts)
├── vite.config.js      # Build tool configuration
├── src/
│   ├── App.jsx         # Main app component
│   ├── CameraView.jsx  # Where the magic happens (camera + AI)
│   └── main.jsx        # App entry point
└── README.org          # This file you're reading
#+END_SRC

---

* For Linux power users: The VLLM option

*This section is for advanced users who want maximum performance on Linux servers or VMs.*

If you're running this on a Linux server, have a good GPU, and want the absolute fastest inference, you can skip Ollama entirely and use VLLM. This is more complex but can be worth it for production deployments.

** What you'll need (Linux only)

- Linux (Ubuntu/CentOS/etc.)
- Python 3.8+
- NVIDIA GPU recommended (but CPU works)
- More technical patience

** Setup

#+BEGIN_SRC bash
# Create a Python virtual environment
python3 -m venv vllm-env
source vllm-env/bin/activate

# Update pip
pip install --upgrade pip setuptools

# Install Rust (VLLM needs it for some dependencies)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env

# Install VLLM and friends
pip install vllm transformers torch
#+END_SRC

** Running the VLLM server

#+BEGIN_SRC bash
# Activate your environment
source vllm-env/bin/activate

# Start the VLLM server with SmolVLM
python -m vllm.entrypoints.openai.api_server \
  --model HuggingFaceTB/SmolVLM-Instruct \
  --port 8000 \
  --served-model-name smolvlm \
  --max-model-len 4096

# For CPU-only (slower but works without GPU)
python -m vllm.entrypoints.openai.api_server \
  --model HuggingFaceTB/SmolVLM-Instruct \
  --port 8000 \
  --served-model-name smolvlm \
  --device cpu
#+END_SRC

** Using with the web app

In the app interface, change:
- Base API: =http://localhost:8000=
- Model: =smolvlm=

** VLLM troubleshooting

*** If Rust compiler fails
#+BEGIN_SRC bash
# Make sure Rust is in your PATH
source ~/.cargo/env
rustc --version

# Restart your terminal if needed
source ~/.bashrc  # or ~/.zshrc
#+END_SRC

*** If installation is still broken
Try pre-built wheels:
#+BEGIN_SRC bash
# For CUDA 11.8
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1  
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121

# For CPU-only
pip install vllm --extra-index-url https://download.pytorch.org/whl/cpu
#+END_SRC

*** Performance monitoring
#+BEGIN_SRC bash
# Check if your GPU is being used
nvidia-smi

# Test the VLLM server
curl http://localhost:8000/v1/models
#+END_SRC

*Why use VLLM?* It's significantly faster than Ollama, uses less VRAM, and is designed for production deployments. But it's Linux-only and more complex to set up.
